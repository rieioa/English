/**
 * Test í˜ì´ì§€ ì»´í¬ë„ŒíŠ¸.
 *
 * ì´ë¯¸ì§€ ì—…ë¡œë“œë¥¼ í†µí•´ ìƒí™©ì„ ë¶„ì„í•˜ê³ ,
 * AIê°€ ìƒì„±í•œ ì˜ì–´ ì„¤ëª…, ì§ˆë¬¸, ìŒì„±(TTS)ì„ ì œê³µí•˜ëŠ” í˜ì´ì§€ì´ë‹¤.
 *
 * ì£¼ìš” ê¸°ëŠ¥:
 * - ì´ë¯¸ì§€ íŒŒì¼ ì—…ë¡œë“œ ë° ë¯¸ë¦¬ë³´ê¸°
 * - FastAPI ë°±ì—”ë“œ(API /api/test/analyze) í˜¸ì¶œ
 * - ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼(ì„¤ëª…, ì§ˆë¬¸) í‘œì‹œ
 * - ìƒì„±ëœ ì˜ì–´ ë¬¸ì¥ì˜ ìŒì„±(TTS) ì¬ìƒ
 */

import { useState, useRef, useEffect } from "react";
import "./test.css";

const API_BASE_URL = "http://127.0.0.1:8000";

// í—¬í¼ í•¨ìˆ˜
function withTimeout(promise, ms, message) {
  let t;
  const timeout = new Promise((_, reject) => {
    t = setTimeout(() => reject(new Error(message)), ms);
  });
  return Promise.race([promise, timeout]).finally(() => clearTimeout(t));
}

function sleep(ms) {
  return new Promise((r) => setTimeout(r, ms));
}

function Test() {
  const [imagePreview, setImagePreview] = useState(null);
  const [question, setQuestion] = useState(null);
  const [audioUrl, setAudioUrl] = useState(null); // GPT ì§ˆë¬¸ TTS
  const [loading, setLoading] = useState(false);

  const [recording, setRecording] = useState(false);
  const [recordedAudioUrl, setRecordedAudioUrl] = useState(null);
  const mediaRecorderRef = useRef(null);
  const recordedChunksRef = useRef([]);

  const [sessionId, setSessionId] = useState(null);
  const [recordedBlob, setRecordedBlob] = useState(null);

  const [feedbackResult, setFeedbackResult] = useState(null);
  const [feedbackAudioUrl, setFeedbackAudioUrl] = useState(null);

  const [status, setStatus] = useState("ì•„ë°”íƒ€ ì—”ì§„ ëŒ€ê¸° ì¤‘...");
  const [isConnected, setIsConnected] = useState(false);
  const [isSdkReady, setIsSdkReady] = useState(false);

  const avatarVideoRef = useRef(null);
  const avatarAudioRef = useRef(null);
  const synthesizerRef = useRef(null);
  const peerConnectionRef = useRef(null);

  // SDK Status ì¡°ì ˆ(ì‹ ê²½ ì•ˆ ì¨ë„ ë¨)
  useEffect(() => {
    const timer = setInterval(() => {
      if (window.SpeechSDK?.AvatarSynthesizer) {
        setIsSdkReady(true);
        setStatus("ì—”ì§„ ë¡œë“œ ì™„ë£Œ");
        clearInterval(timer);
      }
    }, 300);
    return () => clearInterval(timer);
  }, []);

  // ì•„ë°”íƒ€ìš© token ìƒì„± ì‹œë„ (WebRTC ì—°ê²° ì‹œë„)
  const initializeWebRTC = async () => {
    if (!isSdkReady) return;
    setStatus("Azure ì¸ì¦ ë° WebRTC ì—°ê²° ì¤‘...");

    try {
      const tokenResp = await fetch(`${API_BASE_URL}/api/azure/token`);
      if (!tokenResp.ok) throw new Error("get-speech-token failed");
      const { token, region } = await tokenResp.json();

      const iceResp = await fetch(`${API_BASE_URL}/api/azure/ice`);
      if (!iceResp.ok) throw new Error("get-avatar-ice failed");
      const iceRaw = await iceResp.json();

      const iceServers = [
        { urls: iceRaw.Urls, username: iceRaw.Username, credential: iceRaw.Password },
      ];

      const sdk = window.SpeechSDK;
      const speechConfig = sdk.SpeechConfig.fromAuthorizationToken(token, region);
      speechConfig.speechSynthesisVoiceName = "en-US-AvaMultilingualNeural";

      const avatarConfig = new sdk.AvatarConfig("lisa", "casual-sitting");
      const synthesizer = new sdk.AvatarSynthesizer(speechConfig, avatarConfig);
      synthesizerRef.current = synthesizer;

      const pc = new RTCPeerConnection({ iceServers });
      peerConnectionRef.current = pc;
      pc.addTransceiver("video", { direction: "recvonly" });
      pc.addTransceiver("audio", { direction: "recvonly" });

      pc.ontrack = (event) => {
        const stream = event.streams?.[0];
        if (!stream) return;
        if (event.track.kind === "video") avatarVideoRef.current.srcObject = stream;
        if (event.track.kind === "audio") avatarAudioRef.current.srcObject = stream;
      };

      await withTimeout(
        synthesizer.startAvatarAsync(pc),
        20000,
        "startAvatarAsync timeout (20s). ICE/Network/Firewall ê°€ëŠ¥ì„± í¼"
      );

      try { await avatarVideoRef.current?.play?.(); } catch {}
      try { await avatarAudioRef.current?.play?.(); } catch {}

      setIsConnected(true);
      setStatus("âœ… ì•„ë°”íƒ€ ì—°ê²° ì™„ë£Œ");
    } catch (err) {
      console.error(err);
      setStatus(`âŒ WebRTC ì—°ê²° ì‹¤íŒ¨: ${err?.message ?? String(err)}`);
    }
  };

  // ì´ë¯¸ì§€ ì—…ë¡œë“œ â†’ GPT ë¶„ì„ â†’ ì§ˆë¬¸ TTS
  const handleFileChange = async (e) => {
    const file = e.target.files[0];
    if (!file) return;

    setImagePreview(URL.createObjectURL(file));
    setQuestion(null);
    setAudioUrl(null);
    setLoading(true);

    const formData = new FormData();
    formData.append("image", file);

    try {
      const response = await fetch(`${API_BASE_URL}/api/question`, {
        method: "POST",
        body: formData
      });
      const data = await response.json();
      setQuestion(data.question);
      setSessionId(data.session_id || null);
      if (data.audio) {
        setAudioUrl(`data:audio/mp3;base64,${data.audio}`);
      }

      // ì•„ë°”íƒ€ê°€ ì§ˆë¬¸ì„ ë°”ë¡œ ì½ê²Œë”
      if (
        isConnected &&
        typeof data.question === "string" &&
        data.question.trim()
      ) {
        try {
          setStatus("ğŸ—£ï¸ ì•„ë°”íƒ€ê°€ ì§ˆë¬¸ì„ ì½ëŠ” ì¤‘...");
          await speakAvatar(data.question);
          setStatus("ğŸ¤ ì§ˆë¬¸ì„ ë“¤ì—ˆìŠµë‹ˆë‹¤. ë…¹ìŒì„ ì‹œì‘í•˜ì„¸ìš”.");
        } catch (e) {
          console.warn("Avatar question speak failed:", e);
          setStatus("âŒ ì§ˆë¬¸ ë°œí™” ì‹¤íŒ¨");
        }
      }

    } catch (err) {
      console.error(err);
      alert("ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.");
    } finally {
      setLoading(false);
    }
  };

  // ë…¹ìŒë³¸ ë°±ì—”ë“œë¡œ ì „ì†¡ 
  const postFeedback = async (audioBlob) => {
    if (!sessionId) {
      alert("session_idê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.");
      return;
    }

    const fd = new FormData();
    fd.append("session_id", sessionId);
    fd.append("audio", audioBlob, "answer.webm");

    const resp = await fetch(`${API_BASE_URL}/api/feedback`, {
      method: "POST",
      body: fd,
    });

    if (!resp.ok) {
      const errText = await resp.text();
      throw new Error(`${resp.status} ${errText}`);
    }

    const data = await resp.json();

    setFeedbackResult(data.feedback);
    if (data.audio) {
      setFeedbackAudioUrl(`data:audio/mp3;base64,${data.audio}`);
    }

    // ì•„ë°”íƒ€ê°€ í”¼ë“œë°±ì„ ë°”ë¡œ ì½ê²Œë”
    if (typeof data.tts_text === "string" && data.tts_text.trim()) {
      try {
        await speakAvatar(data.tts_text);
      } catch (e) {
        console.warn("Avatar speakTextAsync failed:", e);
      }
    }
    return data;
  };

  // ì•„ë°”íƒ€ ë°œí™” Promise Wrapper
  const speakAvatar = (text) =>
  new Promise((resolve, reject) => {
    const syn = synthesizerRef.current;
    if (!syn) return resolve(false); // ì•„ë°”íƒ€ ì—°ê²° ì•ˆëìœ¼ë©´ ìŠ¤í‚µ

    syn.speakTextAsync(
      text,
      () => resolve(true),
      (err) => reject(err)
    );
  });

  // ë…¹ìŒ ì‹œì‘
  const startRecording = async () => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    const mediaRecorder = new MediaRecorder(stream);
    recordedChunksRef.current = [];

    mediaRecorder.ondataavailable = (e) => {
      if (e.data.size > 0) recordedChunksRef.current.push(e.data);
    };

    mediaRecorder.onstop = async () => {
      const blob = new Blob(recordedChunksRef.current, { type: "audio/webm" });
      const url = URL.createObjectURL(blob);

      setRecordedAudioUrl(url);
      setRecordedBlob(blob);

      try {
        // ë…¹ìŒ ì¢…ë£Œ = ì¦‰ì‹œ feedback ìš”ì²­
        const feedbackData = await postFeedback(blob);
      } catch (e) {
        console.error(e);
        alert("í”¼ë“œë°± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.");
      }
    };

    mediaRecorder.start();
    mediaRecorderRef.current = mediaRecorder;
    setRecording(true);
  };

  // ë…¹ìŒ ì¢…ë£Œ
  const stopRecording = () => {
    mediaRecorderRef.current.stop();
    setRecording(false);
  };

  return (
    <div className="page">
      <nav className="navbar">
        <h2 className="logo">AI Situation Tutor</h2>
        <span className="subtitle">English Learning Helper</span>
      </nav>

      <main className="container">
        {/* ì™¼ìª½: ì´ë¯¸ì§€ ì—…ë¡œë“œ + ë¯¸ë¦¬ë³´ê¸° */}
        <section className="left">
          <div className="upload-card">
            <div className="upload-box">
              <input
                type="file"
                accept="image/*"
                onChange={handleFileChange}
                className="file-input"
              />
              <p className="upload-icon">ğŸ“¸</p>
              <p className="upload-title">ì‚¬ì§„ì„ ì—¬ê¸°ì— ë“œë¡­í•˜ê±°ë‚˜ í´ë¦­í•˜ì„¸ìš”</p>
              <p className="upload-desc">ë¶„ì„í•˜ê³  ì‹¶ì€ ìƒí™©ì˜ ì‚¬ì§„ì„ ì˜¬ë ¤ì£¼ì„¸ìš”</p>
            </div>
          </div>

          {imagePreview && (
            <div className="image-card">
              <img src={imagePreview} alt="preview" />
            </div>
          )}
        </section>

        {/* ì˜¤ë¥¸ìª½: GPT ì§ˆë¬¸ + TTS + ë…¹ìŒ */}
        <section className="right">

          {!loading && !question && !recordedAudioUrl && (
            <div className="empty">
              ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ë©´ ì§ˆë¬¸ê³¼ ë…¹ìŒ ê¸°ëŠ¥ì´ í™œì„±í™”ë©ë‹ˆë‹¤.
            </div>
          )}

          {question && (
            <div className="question-section">
              <h3>â“ Question</h3>
              <p>{question}</p>
            </div>
          )}

          <div className="avatar-view">
            <video ref={avatarVideoRef} autoPlay playsInline />
            <audio ref={avatarAudioRef} autoPlay />
          </div>

          <div className="avatar-connect">
            <button onClick={initializeWebRTC} disabled={isConnected}>
              {isConnected ? "ğŸŸ¢ Avatar Connected" : "ğŸ”Œ Connect Avatar"}
            </button>
          {status && <p>{status}</p>}
          </div>

          {/* ë…¹ìŒ ë²„íŠ¼: í•­ìƒ ìƒë‹¨ */}
          <div className="record-section">
            <h3>ğŸ¤ Record Your Answer</h3>
            {!recording && <button onClick={startRecording}>Start Recording</button>}
            {recording && <button onClick={stopRecording}>Stop Recording</button>}
            {recordedAudioUrl && (
              <div style={{ marginTop: "10px" }}>
                <p>Recorded Audio:</p>
                <audio controls src={recordedAudioUrl} />
              </div>
            )}
          </div>

          {/* ì§ˆë¬¸ + GPT TTS ì˜¤ë””ì˜¤ */}
          {loading && <p>AIê°€ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...</p>}


          {feedbackResult && (
            <div className="feedback-section" style={{ marginTop: "20px" }}>
              <h3>âœ… Feedback</h3>

              {/* Scores */}
              {feedbackResult.scores && (
                <div style={{ display: "flex", gap: "12px", flexWrap: "wrap" }}>
                  <span>Fluency: {feedbackResult.scores.fluency}/5</span>
                  <span>Grammar: {feedbackResult.scores.grammar}/5</span>
                  <span>Appropriateness: {feedbackResult.scores.appropriateness}/5</span>
                </div>
              )}

              {/* Bullets */}
              {Array.isArray(feedbackResult.feedback_bullets) && (
                <ul style={{ marginTop: "10px" }}>
                  {feedbackResult.feedback_bullets.map((b, i) => (
                    <li key={i}>{b}</li>
                  ))}
                </ul>
              )}

              {/* Suggested Answer */}
              {feedbackResult.rewrites?.polite && (
                <div style={{ marginTop: "10px" }}>
                  <h4>ğŸ’¡ Suggested Answer</h4>
                  <p>{feedbackResult.rewrites.polite}</p>
                </div>
              )}
            </div>
          )}
        </section>
      </main>
    </div>
  );
}

export default Test;